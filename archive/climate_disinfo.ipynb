{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2501736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abc69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd7bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_num = {\n",
    "    \"0_not_relevant\": 0,\n",
    "    \"1_not_happening\": 1,\n",
    "    \"2_not_human\": 2,\n",
    "    \"3_not_bad\": 3,\n",
    "    \"4_solutions_harmful_unnecessary\": 4,\n",
    "    \"5_science_unreliable\": 5,\n",
    "    \"6_proponents_biased\": 6,\n",
    "    \"7_fossil_fuels_needed\": 7,\n",
    "}\n",
    "\n",
    "\n",
    "def label_to_class(label: str):\n",
    "    return labels_to_num[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a009195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"quotaclimat/frugalaichallenge-text-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915915a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize(datum):\n",
    "    tokens = tokenizer(\n",
    "        datum[\"quote\"], truncation=True, padding=\"max_length\", max_length=256\n",
    "    )\n",
    "    tokens[\"label\"] = labels_to_num[datum[\"label\"]]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15dbe6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=8):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = output.last_hidden_state[:, 0]  # [CLS] token\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4efa408",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06afc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBERTClassifier(num_labels=8).to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "eval_loader = DataLoader(tokenized[\"test\"], batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccae6eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.608\n",
      "Epoch 2 - Train Loss: 0.907\n",
      "Epoch 3 - Train Loss: 0.555\n",
      "Epoch 4 - Train Loss: 0.317\n",
      "Epoch 5 - Train Loss: 0.152\n",
      "Epoch 6 - Train Loss: 0.081\n",
      "Epoch 7 - Train Loss: 0.045\n",
      "Epoch 8 - Train Loss: 0.032\n",
      "Epoch 9 - Train Loss: 0.025\n",
      "Epoch 10 - Train Loss: 0.019\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        b_input_ids = batch[\"input_ids\"].to(device)\n",
    "        b_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        b_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(b_input_ids, b_attention_mask, labels=b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {avg_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "885f1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"distilbert_climate_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54241b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTClassifier(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBERTClassifier()\n",
    "state_dict = torch.load((\"./distilbert_climate_classifier.pt\"))\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eaae5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                 0_not_relevant       0.75      0.76      0.76       307\n",
      "                1_not_happening       0.71      0.78      0.75       154\n",
      "                    2_not_human       0.66      0.65      0.65       137\n",
      "                      3_not_bad       0.70      0.62      0.66        97\n",
      "4_solutions_harmful_unnecessary       0.69      0.71      0.70       160\n",
      "           5_science_unreliable       0.60      0.66      0.63       160\n",
      "            6_proponents_biased       0.67      0.57      0.61       139\n",
      "          7_fossil_fuels_needed       0.61      0.55      0.58        65\n",
      "\n",
      "                       accuracy                           0.69      1219\n",
      "                      macro avg       0.67      0.66      0.67      1219\n",
      "                   weighted avg       0.69      0.69      0.69      1219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = [\n",
    "    \"0_not_relevant\",\n",
    "    \"1_not_happening\",\n",
    "    \"2_not_human\",\n",
    "    \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\",\n",
    "    \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\",\n",
    "    \"7_fossil_fuels_needed\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in eval_loader:\n",
    "        b_input_ids = batch[\"input_ids\"].to(device)\n",
    "        b_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        b_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits, _ = model(b_input_ids, b_attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_disinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
