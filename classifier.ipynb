{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:07:03.930374Z",
     "start_time": "2025-04-01T03:07:03.925789Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53092a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ede0dab662f9c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:23:45.652661Z",
     "start_time": "2025-04-01T02:23:43.895455Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"quotaclimat/frugalaichallenge-text-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4055891109bcd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:04:38.416349Z",
     "start_time": "2025-04-01T03:04:38.107531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c616215b224748ccacccf2e07382cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1d4b35568f41a69289299e2fdee207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "\n",
    "def tokenize_quotes(example):\n",
    "    return tokenizer(\n",
    "        example[\"quote\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def label_to_int(example):\n",
    "    label_to_int = {\n",
    "        \"0_not_relevant\": 0,\n",
    "        \"1_not_happening\": 1,\n",
    "        \"2_not_human\": 2,\n",
    "        \"3_not_bad\": 3,\n",
    "        \"4_solutions_harmful_unnecessary\": 4,\n",
    "        \"5_science_unreliable\": 5,\n",
    "        \"6_proponents_biased\": 6,\n",
    "        \"7_fossil_fuels_needed\": 7,\n",
    "    }\n",
    "    example[\"label\"] = label_to_int[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "train_ds = dataset[\"train\"].remove_columns(\n",
    "    [\"source\", \"url\", \"language\", \"subsource\", \"id\", \"__index_level_0__\"]\n",
    ")\n",
    "test_ds = dataset[\"test\"].remove_columns(\n",
    "    [\"source\", \"url\", \"language\", \"subsource\", \"id\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "train_ds = train_ds.map(tokenize_quotes).remove_columns([\"quote\"])\n",
    "test_ds = test_ds.map(tokenize_quotes).remove_columns([\"quote\"])\n",
    "\n",
    "train_ds = train_ds.map(label_to_int)\n",
    "test_ds = test_ds.map(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c3541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilroberta-base\", num_labels=8\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=model,\n",
    "    input_size=(32, 512, 512)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c77853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645b3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x74933c3c9040>, <torch.utils.data.dataloader.DataLoader object at 0x7493972bbb30>)\n",
      "Length of train dataloader: 153 batches of 32\n",
      "Length of test dataloader: 39 batches of 32\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42e4ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "EPOCHS = 10\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7ec76e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "RobertaForSequenceClassification (RobertaForSequenceClassification)    [32, 145]            [32, 8]              --                   True\n",
       "├─RobertaModel (roberta)                                               [32, 145]            [32, 145, 768]       --                   True\n",
       "│    └─RobertaEmbeddings (embeddings)                                  --                   [32, 145, 768]       --                   True\n",
       "│    │    └─Embedding (word_embeddings)                                [32, 145]            [32, 145, 768]       38,603,520           True\n",
       "│    │    └─Embedding (token_type_embeddings)                          [32, 145]            [32, 145, 768]       768                  True\n",
       "│    │    └─Embedding (position_embeddings)                            [32, 145]            [32, 145, 768]       394,752              True\n",
       "│    │    └─LayerNorm (LayerNorm)                                      [32, 145, 768]       [32, 145, 768]       1,536                True\n",
       "│    │    └─Dropout (dropout)                                          [32, 145, 768]       [32, 145, 768]       --                   --\n",
       "│    └─RobertaEncoder (encoder)                                        [32, 145, 768]       [32, 145, 768]       --                   True\n",
       "│    │    └─ModuleList (layer)                                         --                   --                   42,527,232           True\n",
       "├─RobertaClassificationHead (classifier)                               [32, 145, 768]       [32, 8]              --                   True\n",
       "│    └─Dropout (dropout)                                               [32, 768]            [32, 768]            --                   --\n",
       "│    └─Linear (dense)                                                  [32, 768]            [32, 768]            590,592              True\n",
       "│    └─Dropout (dropout)                                               [32, 768]            [32, 768]            --                   --\n",
       "│    └─Linear (out_proj)                                               [32, 768]            [32, 8]              6,152                True\n",
       "======================================================================================================================================================\n",
       "Total params: 82,124,552\n",
       "Trainable params: 82,124,552\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.63\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 1995.77\n",
       "Params size (MB): 328.50\n",
       "Estimated Total Size (MB): 2324.34\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = next(iter(train_dataloader))\n",
    "\n",
    "summary(\n",
    "    model=model,\n",
    "    input_data=(datum[\"input_ids\"].to(device), datum[\"attention_mask\"].to(device)),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40abce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7af064fa0f46ecb4cbe91bd53263af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.740 | Test loss: 1.791, Test acc: 0.595%\n",
      "Train loss: 0.985 | Test loss: 1.681, Test acc: 0.665%\n",
      "Train loss: 0.710 | Test loss: 1.621, Test acc: 0.711%\n",
      "Train loss: 0.518 | Test loss: 1.597, Test acc: 0.706%\n",
      "Train loss: 0.365 | Test loss: 1.585, Test acc: 0.720%\n",
      "Train loss: 0.254 | Test loss: 1.580, Test acc: 0.714%\n",
      "Train loss: 0.181 | Test loss: 1.570, Test acc: 0.729%\n",
      "Train loss: 0.133 | Test loss: 1.570, Test acc: 0.723%\n",
      "Train loss: 0.111 | Test loss: 1.567, Test acc: 0.720%\n",
      "Train loss: 0.089 | Test loss: 1.565, Test acc: 0.724%\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(EPOCHS)):\n",
    "    # train step\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, components in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "\n",
    "        # move batch to device and extract components\n",
    "        input_ids = components[\"input_ids\"].to(device)\n",
    "        attention_mask = components[\"attention_mask\"].to(device)\n",
    "        labels = components[\"labels\"].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_logits = model(input_ids, attention_mask).logits\n",
    "        y_preds = torch.softmax(y_logits, dim=1)\n",
    "        predicted_labels = torch.argmax(y_preds, dim=1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_fn(y_logits, labels)\n",
    "        train_loss += loss\n",
    "\n",
    "        # zero out gradients, loss backward, step optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # avg loss per epoch\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # test step\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_dataloader:\n",
    "            # move batch to device and extract components\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = bath[\"labels\"].to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            test_logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "            test_preds = torch.softmax(test_logits, dim=1)\n",
    "            test_labels = torch.argmax(test_preds, dim=1)\n",
    "\n",
    "            # calculate loss & accumulate\n",
    "            test_loss += loss_fn(test_preds, labels)\n",
    "\n",
    "            # calculate accuracy\n",
    "            test_acc += accuracy_fn(labels, test_labels)\n",
    "\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print out what's happening\n",
    "    print(\n",
    "        f\"Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f}, Test acc: {test_acc:.3f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5b75fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"distilroberta_climate_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0c9edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilroberta-base\", num_labels=8\n",
    ")\n",
    "state_dict = torch.load((\"./distilroberta_climate_classifier.pt\"))\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b42c1f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                 0_not_relevant       0.79      0.80      0.79       307\n",
      "                1_not_happening       0.75      0.79      0.77       154\n",
      "                    2_not_human       0.65      0.68      0.66       137\n",
      "                      3_not_bad       0.71      0.72      0.71        97\n",
      "4_solutions_harmful_unnecessary       0.68      0.70      0.69       160\n",
      "           5_science_unreliable       0.64      0.65      0.64       160\n",
      "            6_proponents_biased       0.74      0.65      0.69       139\n",
      "          7_fossil_fuels_needed       0.68      0.60      0.64        65\n",
      "\n",
      "                       accuracy                           0.72      1219\n",
      "                      macro avg       0.71      0.70      0.70      1219\n",
      "                   weighted avg       0.72      0.72      0.72      1219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = [\n",
    "    \"0_not_relevant\",\n",
    "    \"1_not_happening\",\n",
    "    \"2_not_human\",\n",
    "    \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\",\n",
    "    \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\",\n",
    "    \"7_fossil_fuels_needed\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        test_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask).logits\n",
    "        preds = torch.softmax(logits, dim=1)\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "\n",
    "        all_preds.extend(pred_labels.cpu().numpy())\n",
    "        all_labels.extend(test_labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, labels=range(8), target_names=labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_disinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
